{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Подключаем tensorflow и прочие необходимые библиотеки:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.420212300Z",
     "start_time": "2023-06-01T18:41:19.362252900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загружаем датасет, произведение Эдварда Моргана Форстера \"Комната с видом\":"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('a_room_with_a_view.txt', 'https://www.gutenberg.org/cache/epub/2641/pg2641.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.583181400Z",
     "start_time": "2023-06-01T18:41:19.373277Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверяем, что все загрузилось."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 403396 characters\n",
      "﻿The Project Gutenberg eBook of A Room With A View, by E. M. Forster\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere in the United States and\r\n",
      "most other parts of the world at no cost and with almost no restrictions\r\n",
      "whatsoever. You may copy it, give\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:250])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.629488100Z",
     "start_time": "2023-06-01T18:41:19.388920700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.629488100Z",
     "start_time": "2023-06-01T18:41:19.420212300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Перед обучением необходимо преобразовать строки в числовое представление.\n",
    "\n",
    "Слой tf.keras.layers.StringLookup может преобразовывать каждый символ в числовой идентификатор. Просто сначала нужно разделить текст на токены."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.629488100Z",
     "start_time": "2023-06-01T18:41:19.435791900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.629488100Z",
     "start_time": "2023-06-01T18:41:19.451418700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[58, 59, 60, 61, 62, 63, 64], [81, 82, 83]]>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.629488100Z",
     "start_time": "2023-06-01T18:41:19.467454300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Инвертируем это представление и восстанавливаем из него удобочитаемые строки"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.629488100Z",
     "start_time": "2023-06-01T18:41:19.483100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.629488100Z",
     "start_time": "2023-06-01T18:41:19.499442100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "array([b'abcdefg', b'xyz'], dtype=object)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.629488100Z",
     "start_time": "2023-06-01T18:41:19.519445400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.629488100Z",
     "start_time": "2023-06-01T18:41:19.535071700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Преобразуем текстовый вектор в поток индексов символов:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(403396,), dtype=int64, numpy=array([95, 49, 65, ...,  1,  2,  1], dtype=int64)>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.817871800Z",
     "start_time": "2023-06-01T18:41:19.550691100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.833495400Z",
     "start_time": "2023-06-01T18:41:19.676768400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "P\n",
      "r\n",
      "o\n",
      "j\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.833495400Z",
     "start_time": "2023-06-01T18:41:19.692425600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.833495400Z",
     "start_time": "2023-06-01T18:41:19.723666500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "batch метод позволяет легко преобразовать эти отдельные символы в последовательности нужного размера."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'\\xef\\xbb\\xbf' b'T' b'h' b'e' b' ' b'P' b'r' b'o' b'j' b'e' b'c' b't'\n",
      " b' ' b'G' b'u' b't' b'e' b'n' b'b' b'e' b'r' b'g' b' ' b'e' b'B' b'o'\n",
      " b'o' b'k' b' ' b'o' b'f' b' ' b'A' b' ' b'R' b'o' b'o' b'm' b' ' b'W'\n",
      " b'i' b't' b'h' b' ' b'A' b' ' b'V' b'i' b'e' b'w' b',' b' ' b'b' b'y'\n",
      " b' ' b'E' b'.' b' ' b'M' b'.' b' ' b'F' b'o' b'r' b's' b't' b'e' b'r'\n",
      " b'\\r' b'\\n' b'\\r' b'\\n' b'T' b'h' b'i' b's' b' ' b'e' b'B' b'o' b'o' b'k'\n",
      " b' ' b'i' b's' b' ' b'f' b'o' b'r' b' ' b't' b'h' b'e' b' ' b'u' b's'\n",
      " b'e' b' ' b'o' b'f' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.833495400Z",
     "start_time": "2023-06-01T18:41:19.739290100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для обучения понадобится набор данных пар (input, label). Где input и label являются последовательностями. На каждом временном шаге вводом является текущий символ, а меткой является следующий символ.\n",
    "\n",
    "Вот функция, которая принимает последовательность в качестве входных данных, дублирует и сдвигает ее, чтобы выровнять ввод и метку для каждого временного шага:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.833495400Z",
     "start_time": "2023-06-01T18:41:19.770960700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.896422700Z",
     "start_time": "2023-06-01T18:41:19.786620200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'\\xef\\xbb\\xbfThe Project Gutenberg eBook of A Room With A View, by E. M. Forster\\r\\n\\r\\nThis eBook is for the use of'\n",
      "Target: b'The Project Gutenberg eBook of A Room With A View, by E. M. Forster\\r\\n\\r\\nThis eBook is for the use of '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.912037600Z",
     "start_time": "2023-06-01T18:41:19.833495400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Перетасовываем данные и распределяем по пакетам"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.912037600Z",
     "start_time": "2023-06-01T18:41:19.849105400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.912037600Z",
     "start_time": "2023-06-01T18:41:19.865138400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.912037600Z",
     "start_time": "2023-06-01T18:41:19.880793Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:19.912037600Z",
     "start_time": "2023-06-01T18:41:19.896422700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 96) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:22.394119800Z",
     "start_time": "2023-06-01T18:41:19.912037600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:22.456640Z",
     "start_time": "2023-06-01T18:41:22.394119800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "array([35, 80, 27, 27, 12, 82, 80, 13, 47, 50, 74, 21, 91, 19, 35,  7, 65,\n       69, 17, 87, 37, 61, 78, 45, 24, 87, 32, 64,  4, 16, 45, 70, 94, 82,\n       59, 49, 67, 61, 53, 19, 65, 60, 81, 81, 13, 72,  1,  7, 94,  6, 49,\n       51, 80,  3, 12, 31, 18, 13, 14, 29, 20, 15, 49, 61, 16, 34, 22, 36,\n       58, 55, 16, 32, 31, 13, 25, 54, 58, 95, 27, 17, 83, 46, 18, 35, 23,\n       57, 46, 54, 62, 19, 92, 20, 64, 50, 14, 68, 77, 18, 46, 83],\n      dtype=int64)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:22.467678900Z",
     "start_time": "2023-06-01T18:41:22.409736600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 96)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.5642695, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:22.467678900Z",
     "start_time": "2023-06-01T18:41:22.425357900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "95.99245"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:22.467678900Z",
     "start_time": "2023-06-01T18:41:22.440997Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:22.472701900Z",
     "start_time": "2023-06-01T18:41:22.456640Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T18:41:22.543463800Z",
     "start_time": "2023-06-01T18:41:22.472701900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучаем модель, используя 30 эпох."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "62/62 [==============================] - 112s 2s/step - loss: 3.3697\n",
      "Epoch 2/30\n",
      "62/62 [==============================] - 108s 2s/step - loss: 2.4014\n",
      "Epoch 3/30\n",
      "62/62 [==============================] - 118s 2s/step - loss: 2.2236\n",
      "Epoch 4/30\n",
      "62/62 [==============================] - 109s 2s/step - loss: 2.0651\n",
      "Epoch 5/30\n",
      "62/62 [==============================] - 106s 2s/step - loss: 1.9142\n",
      "Epoch 6/30\n",
      "62/62 [==============================] - 106s 2s/step - loss: 1.7808\n",
      "Epoch 7/30\n",
      "62/62 [==============================] - 107s 2s/step - loss: 1.6669\n",
      "Epoch 8/30\n",
      "62/62 [==============================] - 113s 2s/step - loss: 1.5687\n",
      "Epoch 9/30\n",
      "62/62 [==============================] - 108s 2s/step - loss: 1.4855\n",
      "Epoch 10/30\n",
      "62/62 [==============================] - 107s 2s/step - loss: 1.4140\n",
      "Epoch 11/30\n",
      "62/62 [==============================] - 113s 2s/step - loss: 1.3513\n",
      "Epoch 12/30\n",
      "62/62 [==============================] - 106s 2s/step - loss: 1.2952\n",
      "Epoch 13/30\n",
      "62/62 [==============================] - 106s 2s/step - loss: 1.2439\n",
      "Epoch 14/30\n",
      "62/62 [==============================] - 112s 2s/step - loss: 1.1938\n",
      "Epoch 15/30\n",
      "62/62 [==============================] - 114s 2s/step - loss: 1.1448\n",
      "Epoch 16/30\n",
      "62/62 [==============================] - 113s 2s/step - loss: 1.0929\n",
      "Epoch 17/30\n",
      "62/62 [==============================] - 110s 2s/step - loss: 1.0419\n",
      "Epoch 18/30\n",
      "62/62 [==============================] - 115s 2s/step - loss: 0.9876\n",
      "Epoch 19/30\n",
      "62/62 [==============================] - 112s 2s/step - loss: 0.9307\n",
      "Epoch 20/30\n",
      "62/62 [==============================] - 108s 2s/step - loss: 0.8730\n",
      "Epoch 21/30\n",
      "62/62 [==============================] - 108s 2s/step - loss: 0.8078\n",
      "Epoch 22/30\n",
      "62/62 [==============================] - 106s 2s/step - loss: 0.7433\n",
      "Epoch 23/30\n",
      "62/62 [==============================] - 106s 2s/step - loss: 0.6773\n",
      "Epoch 24/30\n",
      "62/62 [==============================] - 109s 2s/step - loss: 0.6084\n",
      "Epoch 25/30\n",
      "62/62 [==============================] - 107s 2s/step - loss: 0.5425\n",
      "Epoch 26/30\n",
      "62/62 [==============================] - 107s 2s/step - loss: 0.4832\n",
      "Epoch 27/30\n",
      "62/62 [==============================] - 107s 2s/step - loss: 0.4226\n",
      "Epoch 28/30\n",
      "62/62 [==============================] - 107s 2s/step - loss: 0.3666\n",
      "Epoch 29/30\n",
      "62/62 [==============================] - 111s 2s/step - loss: 0.3193\n",
      "Epoch 30/30\n",
      "62/62 [==============================] - 110s 2s/step - loss: 0.2772\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T19:36:04.909365800Z",
     "start_time": "2023-06-01T18:41:22.488357700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Следующее делает одношаговый прогноз:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T19:36:04.926509500Z",
     "start_time": "2023-06-01T19:36:04.910375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T19:36:05.002946700Z",
     "start_time": "2023-06-01T19:36:04.928509900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Запускаем в цикле, чтобы сгенерировать текст."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ren taken idea our partition\r\n",
      "approached and beacens life life. To know it’s all;\r\n",
      "have worried about it. She thought it must be near to everyone, and yet Lucy’s ngart! I have just\r\n",
      "say he would not tell her.”\r\n",
      "\r\n",
      "Then they start for this remark in Mr. Getens to wind he wnow what she has writing and draw out it was not\r\n",
      "dislike a man for such an hour andied, and at the Beehive! Would you any you again, and has in ty\r\n",
      "destrict, I am not to talk. I wanted to live and hinder than I\r\n",
      "shall go alo.”\r\n",
      "\r\n",
      "“That place, everyone already,” was her reply? Hurchwy danged her spoke of\r\n",
      "his kepthbods, seements alone saint that his profeed society, and their distribute or room window. At taste nor\r\n",
      "word. But whose exertance with that one cannot seazed Mrs. Butterworth shipped as the\r\n",
      "cart-cool, than to tell he was only her breakfast knew with any one she gakes, we thought the\r\n",
      "roads are safe.” But this was a student of vanop; of “dear of a moneth\r\n",
      "he touched the strapet at the stairs.\r\n",
      "\r\n",
      "“The elartines \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.5548593997955322\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "result = []\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T19:36:07.579030700Z",
     "start_time": "2023-06-01T19:36:05.006938Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
