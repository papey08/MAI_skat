# AIF

## Фундаментальные и перспективные концепции искусственного интеллекта

### lab01 – Градиентный спуск и его модификации

- Выбрать [тестовые функции оптимизации](https://ru.wikipedia.org/wiki/Тестовые_функции_для_оптимизации) (2 шт)
- Запрограммировать собственнуб реализацию классического градиентного спуска
- Запрограммировать пайлайн тестирования алгоритма оптимизации
  - Визуализации функции и точки оптимума
  - Вычисление погрешности найденного решения в сравнение с аналитическим для 
  нескольких запусков
  - Визуализации точки найденного решения (можно добавить анимацию на плюс балл)
- Запрограммировать метод вычисления градиента
  - Передача функции градиента от пользователя
  - Символьное вычисление градиента (например с помощью 
  [sympy](https://www.sympy.org/en/index.html)) (на доп балл)
  - Численная аппроксимация градиента (на доп балл)
- Запрограммировать одну моментную модификацию и протестировать ее
- Запрограммировать одну адаптивную модификацию и протестировать ее
- Запрограммировать метод эфолюции темпа обучения и/или метод выбора 
начального приближения и протестировать их

### lab02 – Глобальная оптимизация и метаэврестические алгоритмы

В Pygmo запрогроммировать две своих тестовых функции и найти их оптимум 3 
разными алгоритмами доступными в библиотеке и получить таблицу сравнения

### lab03 – Оптимизация гиперпараметра

- С помощью [optuna]() взять пример, аналогичный третьему туториалу 
документации, используя sklearn и с другим датасетом, выбрать другие  
алгоритмы классификации и клстеризации не из туториала и визуализировать 
графики для полученного процесса
- В качестве других моделей подойдут любые алгоритмы классификации и регрессии 
из sklearn которые не использовались в туториале
- Использовать 2 разных семплера и прунера
- При процессе оптимизации гиперпараметров использовать общую память через 
PostgreSQL

### lab04 – Метод ядерного сглаживания и EM-алгоритм

- Реализовать метод восстановления плотности вероятности двумя способами:
    - EM-алгоритм
    - Ядерное сглаживание
- Применить данные методы на любом наборе случайных точек
- Реализовать метод Метрополиса-Гастингса и Гибсона для несимметричного 
распределения. Применить два метода на основе той функции плотности, которая 
была восстановлена в прошлом пункте, тем самым получив изначальные точки.

### cp – Facial Expression Recognition
