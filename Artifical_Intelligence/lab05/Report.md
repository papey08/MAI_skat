# Отчёт по лабораторной работе
## Генеративные текстовые нейросети

### Студенты: 

| ФИО           | Группа      | Роль в проекте              | Оценка       |
|---------------|-------------|-----------------------------|--------------|
| Борисов Ян    | М8О-308Б-20 | RNN, отчёт                  |              |
| Зубко Дмитрий | М8О-308Б-20 | LSTM, отчёт                 |              |
| Попов Матвей  | М8О-308Б-20 | Двунаправленная LSTM, отчёт |              |

> *Комментарии проверяющего*

## RNN

## LSTM

Для обучения LSTM выбран роман Рэя Брэдбери "451 градус по Фаренгейту". Была использована токенизация на уровне слов с целью улучшения качества создаваемого текста. Модель прошла 100 обучающих эпох. Каждое последующее слово выбиралось наугад из пяти наиболее вероятных кандидатов, идущих после существующего текста. Пример сгенерированного из 50 слов текста, основанный на слове "Огонь":

```
Огонь того это совершенно стену . В нем люди за только был другой , встала вместо опять удовольствием была был тепло ни и – говорила час , что , представляете ее кивнула : она в порядке . Я стояла же , как птичка в фантастической конуре от его профессиональных дождь ;
```

Текст практически не имеет смысла, возможно, это связано с недостаточным объемом обучающего материала. Однако, из-за ограничений вычислительных ресурсов, обучение модели на больших текстовых массивах становится непрактичным. Например, обучение на одном романе на протяжении 100 эпох заняло - 2 часа.

![image](https://github.com/MAILabs-Edu-2023/ai-lab5-popov-zubko-borisov-team/assets/90476016/ac2660a8-9914-4de9-9047-550885c5958d)

## Двунаправленная LSTM

Для тренировки двунаправленной LSTM был выбран роман Брета Истона Эллиса 
«Американский психопат», полученный с Wikibooks. Было решено применить 
токенизацию по словам, чтобы улучшить качество генерируемого текста. Модель 
обучалась 20 эпох, каждое следующее слово выбирается случайно из 5 наиболее 
вероятных слов, которые могут идти после уже существующего текста. Пример 
сгенерированного текста из 50 слов на основе фразы `My name is Patrick Bateman`:

```
My name is Patrick Bateman is the right commercial Evelyn has to talk Evelyn 
screams up Evelyn says Shes not paying some eerie muscles dont go back or the 
waiter begins Uh yeah Bateman Helga asks her confused cuts me to inspect a 
straight at a passing book But my god Evelyn says No Im
```

В целом, полученный текст соответствует грамматическим правилам 
английского языка, но смысла в нём практически нет. Вероятно это связано с 
малым объёмом текста для обучения. С другой стороны, из-за ограничений в 
вычислительных мощностях не представляется возможным натренировать модель на 
больших объёмах текстах, даже на одном романе обучение 20 эпох заняло 3 часа.

![image](https://github.com/MAILabs-Edu-2023/ai-lab5-popov-zubko-borisov-team/assets/81183518/0ba0fd41-01a1-4eb8-965f-4b901a46e529)

## Вывод
